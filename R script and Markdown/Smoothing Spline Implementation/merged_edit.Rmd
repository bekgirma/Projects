---
title: "Smoothing Splines"
author: "Joaquin Baquerizo, Bekalu Debelu, Leo DiPerna, Minji Kim, Daniel Sabanov"
output:
  beamer_presentation:
    theme: "Rochester"
    colortheme: "beaver"
    fonttheme: "structurebold"
classoption: "aspectratio=169"
fontsize: 8pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo=TRUE, comment=NA, warning=FALSE, message=FALSE, fig.width=8, fig.height=4
)
# knitr::opts_chunk$set(echo = FALSE, comment = NA, warning = FALSE, message = FALSE)
```

## Introduction

Examples of smoothing splines can be found all over, but I found them to be most prevalent in time series data.

- Severity trend for insurance losses typically estimated using loglinear regression of average loss amounts on historical period, which results in a single trend estimate for a block of years.

- If the trend can be assumed to be constant over time, that would be sufficient, but what happens if it is not? 

- Ok, so lets use the actual year-to-year changes to produce an inflation trend index.

- The problem with this is that the year-to-year changes can be highly volatile and produce unusual patterns if used in pricing.

- This is when smoothing splines come in for the rescue, since it provides an easy tool or a compromise between a single trend for all years and different trend for each year.

- Of course this can be exemplified to a number of cases such as stocks, interest rates, and in our case sports data. 

## Introduction

Basically what Smoothing Splines does is it overfits the data by having a function $g(x)= (y_{i} - g_{xi})^2$, but then we add a "smoothing" parameter which is basically a penalization term.

- Penalization parameter: $\lambda \int g^"(t)^2 dt$

- The notation $g^"(t)$ indicates the second derivative of the function g. 

- The first derivative of $g(t)$ measures the slope of a function at t, and the second derivative corresponds to the amount y which the slope is changing. 

- Hence, broadly speaking, the second derivative is a measure of its _roughness_: it is large in absolute value if g(t) is very wiggly near t, and it is close to zero otherwise

- The second derivative of a straight line is zero; note that a line is perfectly smooth

## Introduction

- When $\lambda = 0$, then the penalty term ($\lambda \int g^"(t)^2 dt$) will have no effect, and so the function g will be very jumpy and will exactly interpolate the training observations. 

- When $\lambda = \infty$, g will be perfectly smooth - it will just be a straight line that passes as closely as possible to the training points. 

- In fact, g would just be the linear least squares line, since the loss funciton ( _penalty function_ ) amounts to minimizing the residual sum of squares.

- For an intermediate value of $\lambda$ we see that g will approximate the training observations but will be somewhat smooth.

- So, we see that $\lambda$ controls the Bias-Variance trade off in Smoothing Spline

#Comparison of Smoothing Spline to LOESS
```{r}
library(stats)
library(rbenchmark)
players <- read.csv("data/NBA_players.csv")

benchmark("Smoothing Spline" = {
  ss.fit.1 <- smooth.spline(players$MP_PER_G, players$PF_PER_G)},

          "Loess" = {loess.fit <-loess(PF_PER_G ~ MP_PER_G, data = players)},
  replications = 100,
  columns = c("test", "replications", "elapsed","relative"))
```
This benchmark test shows the two functions runtime performance when fitting the
same data 100 times. From the results we can see that smoothing spline is indeed
the faster of the two methods with Loess being about three times as slow

## About Data
The data was collected using R library(MASS). It is a simulated motorcycle accident. Using to test crash helemts, it measures head acceleration in a simulated motorcycle accident.
```{r}
library(MASS)
library(ggplot2)
data("mcycle")
head(mcycle)
```

## Plot of the data 
```{r echo=FALSE}
library(MASS)
library(ggplot2)
data("mcycle")
ggplot(mcycle, aes(times, accel)) + geom_point() + theme_bw() +
  labs(title = "mcycle Acceleration vs Time", x="Time (ms after impact)", y="Acceleration (g)")
```

## Creating a Smoothing Spline
```{r echo=TRUE}
library(stats)
mfit.1 <- smooth.spline(mcycle$times, mcycle$accel)
```

## Smoothing Spline Plot
```{r echo=FALSE}
mfit.1 <- smooth.spline(mcycle$times, mcycle$accel)

ss.df <- data.frame(mfit.1$x, mfit.1$y)
ggplot(mcycle, aes(x=times, y=accel)) + geom_point() + theme_bw() +
  geom_line(data=ss.df, aes(x=mfit.1.x, y=mfit.1.y)) +
  labs(title = "mcycle Acceleration vs Time", x="Time (ms after impact)", y="Acceleration (g)")
```


## Changing Lambda
```{r echo = TRUE}
mfit.2 <- smooth.spline(mcycle$times, mcycle$accel, lambda = 0)

mfit.3 <- smooth.spline(mcycle$times, mcycle$accel, lambda = 100)
```


## Comparing Lambdas
```{r echo=FALSE}
# plot of mcycle and smoothing spline 
# plot(mcycle$times, mcycle$accel, main = "Smoothing splines of mcycle", xlab = "Times", ylab = "Accel")
mfit.1 <- smooth.spline(mcycle$times, mcycle$accel)
# lines(mfit.1, col = "red", lwd = 2)

mfit.2 <- smooth.spline(mcycle$times, mcycle$accel, lambda = 0)
# lines(mfit.2, col = "blue", lwd = 2)

mfit.3 <- smooth.spline(mcycle$times, mcycle$accel, lambda = 10000)
# lines(mfit.3, col = "green", lwd = 2)

# legend('bottomright', lty = 1, col = c('red', 'blue', 'green'), c("lambda = auto", "lambda = 0", "lambda = 100"))

ss.df <- data.frame(x=rep(mfit.1$x, 3),
                    y=c(mfit.1$y, mfit.2$y, mfit.3$y),
                    group=c(rep("auto", length(mfit.1$x)),
                               rep("0", length(mfit.1$x)),
                               rep("10000", length(mfit.1$x))
                               )
                    )
ggplot(data=mcycle, aes(x=times, y=accel)) + geom_point() + theme_bw() +
  geom_line(data=ss.df, aes(x=x, y=y, group=group, color=group)) +
  labs(title="mcycle Acceleration vs Time", x="Time (ms after impact)", y="Acceleration (g)", color="Lambda")
```

## Best Fit Smoothing Spline
```{r echo = TRUE}
mfit.1$lambda
```
Lambda of this data is 0.00011075. This lambda is the best fit smoothing spline for this data. As lambda = 0, the smoothing spline curve is very noisy and overfitting. As lambda = 100, the smoothing spline curve is similar to the simple linear regression and underfitting. 

## Mathematical Background

A \textbf{spline} is a piecewise polynomial with pieces defined by a sequence
of \textbf{knots}

$$
\xi_1 < \xi_2 < ... < \xi_k
$$

such that the pieces join smoothly at the knots.

&nbsp;



## Penalized Sum of Squares

The goal of simple linear regression is to choose $\hat{\beta}$ such that
$y_i = \hat{f}(x_i) = \hat{\beta}_0 + \hat{\beta}_1x_i$ minimizes

$$
\text{SSE} = \sum_{i=1}^{n} \left(y_i-\hat{f}(x_i)\right)^2
$$

&nbsp;

Similarly, when we fit a smoothing spline, we want to minimize the sum of
squares error, but we also want to add a \textbf{smoothing penalty} to ensure a
smooth fit. This penalty has the form

$$
\text{P} = \lambda \int \hat{f}''(x)^2 dx
$$

&nbsp;

$\lambda$ is a \textbf{tuning parameter} that can be adjusted to change the
severity of the smoothing penalty.

## Penalized Sum of Squares

This leads us to the \textbf{penalized sum of squares}, which is what we are
trying to minimize when we fit a smoothing spline. It has the form

$$
\text{PSS} = \text{SSE} + \text{P} = \sum_{i=1}^{n} \left(y_i-\hat{f}(x_i)\right)^2 + \lambda \int \hat{f}''(x)^2 dx
$$

&nbsp;

As $\lambda \rightarrow 0$, the smoothing penalty gets very small and the
resulting curve becomes very noisy, following every detail in the data
(\textbf{overfitting}).

&nbsp;

As $\lambda \rightarrow \infty$, the smoothing penalty gets very large and the
resulting curve looks like something you would get from simple linear
regression (\textbf{underfitting}).

## Minimizing the Penalized Sum of Squares

It turns out that the smoothing penalty can be written in the quadratic form as

$$
\lambda \int \hat{f}''(x)^2 dx = \lambda \mu^\text{T}K\mu
$$

where

```{=latex}
\begin{equation*}
  \mu = 
  \begin{bmatrix}
    \hat{f}(x_1) \\
    \vdots \\
    \hat{f}(x_n)
  \end{bmatrix}
  \qquad\qquad
  K = \Delta^\text{T}W^{-1}\Delta
\end{equation*}
```

&nbsp;

$\Delta$ is an $n \times (n - 2)$ matrix where \
$\Delta_{i,i} = \frac{1}{h_i}$,
$\Delta_{i,i+1} = -\frac{1}{h_i}-\frac{1}{h_i+1}$,
$\Delta_{i,i+2} = \frac{1}{h_i+1}$

&nbsp;

$W$ is an $(n - 2) \times (n - 2)$ symmetric tridiagonal matrix where \
$W_{i-1,i} = W_{i,i-1} = \frac{h_i}{6}$,
$W_{i,i} = \frac{h_i+h_{i+1}}{3}$

&nbsp;

$h_i = \xi_{i+1} - \xi_i$, the distance between consecutive knots

## Minimizing the Penalized Sum of Squares

Now we can write the penalized sum of squares using matrices and vectors as

$$
\text{PSS} = (y - \mu)^\text{T}(y - \mu) + \lambda \mu^\text{T}K\mu
$$

We can take the partial derivative with respect to $\mu$ and set it equal to 0:

$$
\frac{\partial\text{PSS}}{\partial\mu} = -2(y - \mu) + 2\lambda K\mu = 0
$$

Then we can solve for $\mu$:

```{=latex}
\vspace{-2em}
\begin{align*}
  \lambda K\mu &= y - \mu \\
  \mu + \lambda K\mu &= y \\
  (I + \lambda K)\mu &= y \\
  \mu &= (I + \lambda K)^{-1}y
\end{align*}
```

We can use this formula to calculate the fitted values for smoothing splines.

## Data Background

The data was collected using web scrapping sourced from Sports Reference. The 
website includes data collected on different players and sports. Our data in
particular was collected on basketball players from NBA. The data is accurate
up-to the spring semester of 2022.

## Dictionary

Column Name   Definition
-----------   ----------
Name          Player Name
G             Games Played
GS            Games Started
MP            Minutes Played
FG            Field Goals
FGA           Field Goals Attempted
FG2           2 Point Field Goals
FG2A          2 Point Field Goals Attempted
FG3           3 Point Field Goals
FG3A          3 Point Field Goal Attempted
FT            Field Throws
FTA           Field Throws Attempted
ORB           Offensive Rebounds
DRB           Defensive Rebounds

## Dictionary (Continued)

Column Name   Definition
-----------   ----------
TRB           Total Rebounds
AST           Assists Per Game
STL           Steals
BLK           Blocks
TOV           Turnovers
PF            Personal Fouls
PTS           Points
EFG_PCT       Effective Field Goal Percentage
X_PER_G       X Per Game
X_PCT         X Percentage

## What Are We Focusing On?
As you can see there are a lot of variables in this data set and many different analyses can be performed. For now we would like to build a model to predict the number of fouls based on the time each player spends playing.

## Preview
```{r echo=FALSE}
#Change the read csv to your specific file path
players <- read.csv("Data/NBA_players.csv")
head(players)[,c(1,2,3,4,25)]
```

## First Look
```{r, echo=F}
library(ggplot2)
library(gridExtra)
players <- read.csv("data/NBA_players.csv")
ggplot(players, aes(MP_PER_G, PF_PER_G)) +
  geom_point() + 
  ggtitle("Fouls vs. Minutes Played") +
  ylab("Personal Fouls Per Game") +
  xlab("Minutes Played Per Game") +
  theme_bw()
```

## Creating a Model
There is a built in function that performs smoothing splines in the 'stats' library. A nice thing about the built-in function is its ability to select the best lambda for us.

```{r}
  ss.fit.1 <- smooth.spline(players$MP_PER_G, players$PF_PER_G)
```

```{r}
ggplot(data = players, mapping = aes(x = MP_PER_G, y = PF_PER_G)) +
  geom_point() +
  geom_smooth(formula = y ~x, method = loess, se = F) +
  theme_bw()
```

If we were doing it "by hand" it would look like this
```{r}
# This will be used for manual model creation.
```


## Smoothing Spline Plot

```{r, echo=F}
plt.1 <- ggplot(data = players, mapping = aes(x = MP_PER_G, y = PF_PER_G)) +
  geom_point() +
  geom_line(mapping = 
              aes(x = MP_PER_G, 
                  y = predict(ss.fit.1, players$MP_PER_G)$y,
                  color="red"), 
            show.legend = F, 
            linewidth=1.5) +
  ggtitle("Fouls vs. Minutes Played - Smoothing Splines") +
  ylab("Personal Fouls Per Game") +
  xlab("Minutes Played Per Game") +
  theme_bw()

#Add different fits with different lambdas to show effect
```

#Comparing Smoothing Spline to Loess
```{r, echo=FALSE}
plt.loess <- ggplot(data = players, mapping = aes(x = MP_PER_G, y = PF_PER_G)) +
  geom_point() + 
  geom_smooth(formula = y ~ x, method = "loess", se = F) +
  ggtitle("Fouls vs. Minutes Played - Loess") +
  ylab("Personal Fouls Per Game") +
  xlab("Minutes Played Per Game") +
  theme_bw()

grid.arrange(plt.1, plt.loess, nrow = 1)
```

## Changing our lambda
```{r}
ss.fit.2 <- 
  smooth.spline(players$MP_PER_G, 
                players$PF_PER_G, 
                lambda=0)

ss.fit.3 <- 
  smooth.spline(players$MP_PER_G, 
                players$PF_PER_G, 
                lambda=10000)
```

## Changing our lambda
```{r echo= FALSE}
plt.lm <- ggplot(data = players, mapping = aes(x = MP_PER_G, y = PF_PER_G)) +
  geom_point() +
  geom_smooth(formula = y ~ x, method = 'lm', se = F) +
  ggtitle("Ordinary Least Squares") +
    xlab("Minutes Played Per Game")+
    ylab("Personal Fouls Per Game")

plt.3 <- ggplot(data = players, mapping = aes(x = MP_PER_G, y = PF_PER_G)) +
  geom_point() +  
    geom_line(mapping = 
              aes(x = MP_PER_G, 
                  y = predict(ss.fit.3, players$MP_PER_G)$y), color = "red", linewidth=1) + 
    ggtitle("Lambda = 10000 ") +
    xlab("Minutes Played Per Game")+
    ylab("Personal Fouls Per Game")

grid.arrange(plt.lm, plt.3, nrow = 1)
```

## Comparing Lambdas
```{r, echo=F}
plt.1 <- ggplot(data = players, mapping = aes(x = MP_PER_G, y = PF_PER_G)) +
  geom_point() +
  geom_line(mapping = 
              aes(x = MP_PER_G, 
                  y = predict(ss.fit.1, players$MP_PER_G)$y,
                  color = "Auto"), show.legend = F, linewidth=1)  + 
    ggtitle("Lambda = Auto") +
    ylab("")
  
plt.2 <- ggplot(data = players, mapping = aes(x = MP_PER_G, y = PF_PER_G)) +
  geom_point() +
    geom_line(mapping = 
              aes(x = MP_PER_G, 
                  y = predict(ss.fit.2, players$MP_PER_G)$y,
                  color = "0"), show.legend = F, linewidth=1)  + 
    ggtitle("Lambda = 0") +
    ylab("Personal Fouls Per Game")

plt.3 <- ggplot(data = players, mapping = aes(x = MP_PER_G, y = PF_PER_G)) +
  geom_point() +  
    geom_line(mapping = 
              aes(x = MP_PER_G, 
                  y = predict(ss.fit.3, players$MP_PER_G)$y), color = "red", linewidth=1) + 
    ggtitle("Lambda = 10000 ") +
    xlab("Minutes Played Per Game")+
    ylab("")


#Add different fits with different lambdas to show effect
grid.arrange(plt.1, plt.2, plt.3)
#Add different fits with different lambdas to show effect
```
